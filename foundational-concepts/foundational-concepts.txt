
---------------------
-- Data Lifecycle
---------------------

Steps of the Data Lifecycle

- For the exam, we should be familiar with each step of the lifecycle
    and what GCP services are used in those steps

Ingest
    - pull in raw data
    - streaming or batch
    - initial validation

Store
    - Databases, object store
    - reliable and accessible
    - structured vs unstructured

Process
    - Clean, normalize, enrich
    - prepare for analysis

Analyze
    - Explore & Visualize
    - Generate Insights
    - Apply ML

Extract Value
    - Take Action
    - Inform Business Decisions
    - Share with stakeholders

- CENTRAL -
Governance
    - Privacy, Compliance
    - Data quality
    - Security


- Services

Ingest
    - Cloud Pub/Sub (Streaming)
    - Cloud Dataflow (Beam, ingest stream or batch)
    - Cloud IoT Core (ingest data from IoT)
    - Logging
    - Cloud Storage

Store
    - Cloud Storage (storing any data, esp unstructured)
    - BigQuery (structured data)
    - Cloud BigTable (noSQL wide data)
    - Cloud MemoryStore (in-memory storage)
    - Cloud SQL (managed RDBMS)
    - Cloud Spanner (RDBMS with high scalability and availability)
    - Firestore (document db used for application and user data)

Process
    - Dataproc (managed spark hadoop hive service for large-scale data processing)
    - BigQuery (contributes to processing phase by allowing users to run transformation)
    - Cloud Data Fusion (UI-based ETL)
    - Data Loss Prevention (identify, classify, and protect sensitive data)
    - Cloud Dataflow (batch & streaming data)
    - Cloud Dataprep (visually exploring, cleaning, normalizing data)
    - Cloud Functions
    - Composer (orchestrate, automate processing tasks)

Analyze and Extract Value
    - BigQuery
    - Looker
    - Vertex AI (develop models, run analysis using jupyter notebooks)

Governance
    - Cloud IAM
    - Cloud Logging
    - Data Catalog (metadata mgmt tool)
    - Dataplex
    - DLP API
    - VPC
    - KMS


-----------------------------
-- Structured, Unstructured, 
-- and Semi-Structured Data
-----------------------------

Structured Data
    - highly organized, easily processed. adheres to a schema. 
    - tabular, row-column format
    - usually stored in RDBMS
    - example: financial data, student or employee info

Unstructured Data
    - text-based unstructured data, image, audio
    - "free-form" in nature, lacks a predefined schema, wide range of types and formats
    - requires specialized tools to analyze, such as:
        .- Natural Language Processing
        .- Image Processing Models

Semi-Structured Data
    - key-value like JSON, YAML, XML files
    - flexible format, often contains tags or attributes for organizing
    - no fixed schema, but some metadata that describes the data

- GCP Services

Structured Data
    - BigQuery
    - Cloud SQL
    - Cloud Spanner

Unstructured Data
    - Cloud Storage

Semi-structured Data
    - BigTable
    - Firestore
    - Memorystore


-----------------------------
-- Batch vs Streaming Data
-----------------------------

Batch
    - processed in large, scheduled chunks
    - bounded data
    - Cloud Dataproc, Cloud Composer, BigQuery, Cloud Dataflow

Streaming
    - processed continuously, in real-time or near real-time
    - unbounded data
    - Cloud Pub/Sub, BigQuery, Cloud Dataflow


-----------------------------
-- Types of Databases
-----------------------------

Relational/SQL
    - stores data in highly structured format
    - MySQL, PostgreSQL, Oracle, SQL Server

Non-relational/NoSQL
    - stores data for unstructured, semi structured
    - designed for scalability and flexibility
    - MongoDB, cassandra


-----------------------------
-- Management Levels in GCP
-----------------------------

Unmanaged Services
    - GCP provides the infrastructure, but you manage it entirely
    - you consider scaling, how your servers should grow, or scale down when things are quiet
    - Networking - subnet, IP Addresses

Managed Services
    - GCP takes responsibility for managing the infrastructure, including
        server setup, software, and OS maintenance
    - you control how your application is configured and deployed
    - App Engine, Google Kubernetes Engine, Cloud BigTable

No-Ops (Serverless) Services
    - GCP automatically manages infrastructure and servers, including scaling
    - all no-ops services are also managed, but not all managed services are no-ops
    - highest level of abstraction
    - you focus on the code, data, or application logic
    - Pub/Sub, Cloud Functions, Cloud Run, Dataflow


-----------------------------
-- Types of Scaling
-----------------------------

Horizontal Scaling
    - adding more servers, instances, or nodes to distribute workload
Vertical Scaling
    - adding more compute (CPU/GPU/RAM) on the current servers


-----------------------------
-- Big Data Tools
-----------------------------

-Principles of Big Data Tools

Distributed/Parallel Processing
    - distributing load and processing them in parallel
    - allows faster processing and caters to large volumes of data

Scalability
    - ability to expand or contract depending on the needed load at a point in time
    - could either be horizontal or vertical

Fault Tolerance
    - in distributed systems, faults are inevitable, either due to nodes going down or a network issue
    - if one part fails, the others continue to work smoothly

As cloud platforms grew, many started offering managed versions of these open-source tools (like spark, beam, etc)

Apache Big Data Tools and their GCP versions

Apache Kafka -> Cloud Pub/Sub
Apache Beam -> Cloud Dataflow
Hadoop,SpaRK -> Cloud Dataproc
Apache Airflow -> Cloud Composer
Apache HBase -> Cloud Bigtable
Apache Hive -> BigQuery


-----------------------------
-- ML Development Lifecycle
-----------------------------

Data Collection
    - gather data from various sources (databases, APIs)
    - gather relevant and quality data to train our ML models

Data Processing
    - clean and transform gathered data

Train/Test Split
    - define data into two sets: one for training, one for testing

Model Training & Validation
    - feed training data into ML algorithm
    - validate model to fine-tune parameters

Model Evaluation
    - using testing data, assess the models performance. check if it is ready for deployment

Deployment
    - deployed into production environment where it is used for predicting
    - continuously monitor to ensure that it still can predict as the data evolves


-----------------------------
-- Regularization in ML
-----------------------------

Overfitting
    - happens when a model becomes too complex and starts memorizing the training data
    - this wont perform well on new and/or unseen data

Regularization to reduce overfitting=
    - technique that adds a penalty to the model's complexity to discourage overfitting
    - the goal is to balance simplicity with ability to capture patterns in the training dataset
        enabling it to generalize better on unseen data

- Types of Regularization

L1 Regularization (Lasso)
    - reduces the number of features
L2 Regularization (Ridge)
    - manages the size of weights


-----------------------------
-- ETL and ELT
-----------------------------

ETL, extract-transform-load. a more traditional approach where we transform before processing.
ELT, extract-load-transform. modern approach, allows a faster data ingestion since transform happens post-load
                                and can be done on demand depending on what users need


-----------------------------
-- Throughput and Latency
-----------------------------

Throughput
    - amount of data processed over a period of time
    - often measured in gigabytes per second (GBps) or records per second

Latency
    - refers to the delay between the time data is ingested and when it becomes available for querying or further processing
    - low latency means there is minimal delay, data is quickly available for use after ingested

high throughput != low latency

high throughput + high latency
    - a system can process large volumes of data but still takes time to make data available for use

low latency + low throughput
    - low latency means data is available quickly but system may struggle to handle large volumes of data


-----------------------------
-- Availability
-----------------------------

Availability refers the the ability of a system to remain operational and accessible even during failures or disruptions

importance of availability
    - ensures that users can always access data or services
    - needed for mission-critical systems
    - downtime affects business continuity and user satisfaction

- Measuring Availability

commonly measured in uptime percentages (e.g. 99.99% uptime)
the higher the percentage, the lower the downtime allowed over a given period
SLAs (Service Level Agreements) guarantee specific availability levels, 
    defining the maximum available downtime

99.5% availability -> 1.8 days of downtime per year
99.99% availability -> 52.56 minutes of downtime per year
99.999% (five nines) -> 5.26 minutes of downtime per year

(online calculator for this lol https://uptime.is/99.99)

Cloud Spanner guarantees five nines, as it emphasizes extremely high global availability


-----------------------------
-- Regions and Zones
-----------------------------
us-central1-a (u know this)

us-central1 - region
a (also has b, c) - zone


Regions are large geographical areas, and inside it, GCP has multiple Zones

Regions
    - large geographic area with multiple zones (data center clusters)
    - resources in the same region communicate quickly and cost-effectively

Zones
    - independent data center (or several)
    - spreading workloads across zones improves fault tolerance - other zones remain
        active if ever one of the zone fails


- Regions and Zones in Practice

Cloud Storage
    - Buckets can be regional, dual-region, or multi-regional

BigQuery
    - datasets can be regional or multi-regional

Cloud SQL
    - instances are zonal

Dataproc
    - clusters are zonal

You may assign a default region and zone for your GCP Project


-----------------------------
-- ACID compliance
-----------------------------

A - Atomicity
    - ensures all transactions are "all or nothing". Ensures that all transactions must be completed entirely or none at all
    - if one part fails, the entire transaction fails, and the database remain unchanged
    - this avoids partial updates that leaves the system in an inconsistent state

C - Consistency
    - ensures that data integrity is preserved
    -  any data written to the database must be valid according to all defined rules, including constraints, cascades, triggers

I - Isolation
    - transactions do not interfere with each other
    - even when multiple transactions run concurrently, each runs as if theyre the only one

D - Durability
    - data is permanent
    - once a transaction has been committed, it will remain in the system, 
        even in the event of a system crash or power failure

- Consistency
Data Consistency ensures that all users see the same, correct version of the data
Consistency is evaluated based on two criteria:
    - timeliness: whether the data is up-to-date
    - order: whether the order of the data updates is preserved

Strong Consistency vs Eventual Consistency
ACID requires strong consistency

*from quiz*
Strong consistency ensures that all queries always return the most up-to-date and accurate version of the data,
making it essential for critical applications like financial transactions. 
The tradeoff is slower query responses because strict synchronization is required to maintain consistency across distributed systems.

returning outdated data improves performance but does not guarantee correctness,
making it unsuitable for strong consistency. Allowing updates to occur out of order can lead to temporary 
inconsistencies, which contradicts the principle of Strong Consistency. Delaying consistency over time
refers to eventual consistency, which prioritizes availability over immediate accuracy.




